{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPErLWN/FK4HDRFvpp0rgR1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saint-broseph/ai-portfolio-optimization-lstm/blob/main/aivsmvo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCDsTJoVW3PB"
      },
      "outputs": [],
      "source": [
        "!pip install yfinance PyPortfolioOpt matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "tickers = [\"BTC-USD\", \"ETH-USD\", \"SPY\", \"GLD\", \"NVDA\"]\n",
        "df = yf.download(tickers, start=\"2019-01-01\", end=\"2024-12-31\", auto_adjust=True)\n",
        "\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    data = df['Close']\n",
        "else:\n",
        "    data = df\n",
        "data = data.dropna()\n",
        "returns = data.pct_change().dropna()\n",
        "\n",
        "print(f\"Successfully downloaded data for: {list(data.columns)}\")\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "YOFt8MF1Xfvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypfopt.efficient_frontier import EfficientFrontier\n",
        "from pypfopt import risk_models\n",
        "from pypfopt import expected_returns\n",
        "import matplotlib.pyplot as plt\n",
        "# 1. Calculate the expected annual returns and the annual sample covariance matrix\n",
        "# This tells the math model how much each asset earns and how risky/volatile it is\n",
        "mu = expected_returns.mean_historical_return(data)\n",
        "S = risk_models.sample_cov(data)\n",
        "# 2. Optimize for the \"Max Sharpe Ratio\" (the best return-to-risk balance)\n",
        "ef = EfficientFrontier(mu, S)\n",
        "weights = ef.max_sharpe()\n",
        "cleaned_weights = ef.clean_weights()\n",
        "print(\"--- Traditional MVO Weights ---\")\n",
        "for asset, weight in cleaned_weights.items():\n",
        "    print(f\"{asset}: {weight*100:.2f}%\")\n",
        "# 3. Display Performance\n",
        "print(\"\\n--- Expected Performance ---\")\n",
        "performance = ef.portfolio_performance(verbose=True)"
      ],
      "metadata": {
        "id": "OmP_WznNXn4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def create_features(df):\n",
        "    # Calculate daily returns\n",
        "    returns = df.pct_change()\n",
        "    # Calculate 14-day RSI (Relative Strength Index) - a momentum indicator\n",
        "    delta = returns.diff()\n",
        "    up = delta.clip(lower=0)\n",
        "    down = -1 * delta.clip(upper=0)\n",
        "    ema_up = up.ewm(com=13, adjust=False).mean()\n",
        "    ema_down = down.ewm(com=13, adjust=False).mean()\n",
        "    rs = ema_up / ema_down\n",
        "    rsi = 100 - (100 / (1 + rs))\n",
        "    # Calculate 20-day Volatility\n",
        "    vol = returns.rolling(window=20).std()\n",
        "    return returns, rsi, vol\n",
        "returns, rsi, vol = create_features(data)\n",
        "# Combine them into a single dataset and drop the \"NaN\" rows from the beginning\n",
        "# We will use these to feed the LSTM in the next step\n",
        "features = pd.concat([returns, rsi, vol], axis=1).dropna()\n",
        "print(f\"Features created. New shape: {features.shape}\")"
      ],
      "metadata": {
        "id": "tn-FPBK3Xsep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def calculate_max_drawdown(portfolio_returns):\n",
        "    # Fix: Convert numpy array to pandas Series if needed\n",
        "    if isinstance(portfolio_returns, np.ndarray):\n",
        "        portfolio_returns = pd.Series(portfolio_returns)\n",
        "\n",
        "    # Calculate cumulative wealth (starting at 1.0)\n",
        "    cumulative_returns = (1 + portfolio_returns).cumprod()\n",
        "\n",
        "    # Calculate the peak wealth at each point in time using cummax()\n",
        "    # (cummax is faster and more reliable than expanding().max())\n",
        "    peak = cumulative_returns.cummax()\n",
        "\n",
        "    # Calculate the percentage drop from the peak\n",
        "    drawdown = (cumulative_returns / peak) - 1\n",
        "    return drawdown.min()\n",
        "\n",
        "# Calculate MDD for an equal-weighted portfolio as a quick test\n",
        "equal_weights = np.array([1/len(tickers)] * len(tickers))\n",
        "test_port_returns = (returns.dropna() * equal_weights).sum(axis=1)\n",
        "print(f\"Max Drawdown of a simple Equal-Weighted Portfolio: {calculate_max_drawdown(test_port_returns):.2%}\")"
      ],
      "metadata": {
        "id": "_v557RbFXw-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 1. Scale all features (Returns, RSI, Volatility)\n",
        "scaler = MinMaxScaler()\n",
        "scaled_features = scaler.fit_transform(features)\n",
        "\n",
        "# 2. Create Sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i : (i + seq_length)]\n",
        "        # Target: The actual returns of the assets on the very next day\n",
        "        y = data[i + seq_length, :len(tickers)]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\n",
        "\n",
        "seq_length = 30 # Looking back 30 trading days\n",
        "X, y = create_sequences(scaled_features, seq_length)\n",
        "\n",
        "# 3. Split: 80% to train the AI, 20% to test it on \"unseen\" future data\n",
        "split = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:split], X[split:]\n",
        "y_train, y_test = y[:split], y[split:]\n",
        "\n",
        "# Convert to PyTorch Tensors (The format the AI brain speaks)\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "print(f\"Training shape: {X_train.shape}\") # Should be [Samples, 30, Number_of_Features]"
      ],
      "metadata": {
        "id": "iUHwDgTHYxrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMRebalancer(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(LSTMRebalancer, self).__init__()\n",
        "        # The LSTM layer processes the time-series \"memory\"\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        # The Linear layer turns that memory into a portfolio decision\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        # Softmax ensures the weights are positive and sum to 1.0 (100%)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hn, _) = self.lstm(x)\n",
        "        out = self.fc(hn[-1]) # Use the last hidden state\n",
        "        return self.softmax(out)\n",
        "\n",
        "# Initialize\n",
        "# input_size = number of columns in your features (Returns + RSI + Vol)\n",
        "model = LSTMRebalancer(input_size=X_train.shape[2], hidden_size=64, output_size=len(tickers))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "jhCxot6AY4zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_drawdown_loss(predicted_weights, actual_returns):\n",
        "    # Calculate portfolio return\n",
        "    portfolio_return = torch.sum(predicted_weights * actual_returns, dim=1)\n",
        "\n",
        "    mean_return = torch.mean(portfolio_return)\n",
        "\n",
        "    # Check if we actually have negative returns to penalize\n",
        "    negative_returns = portfolio_return[portfolio_return < 0]\n",
        "    if len(negative_returns) > 0:\n",
        "        risk_penalty = torch.mean(torch.square(negative_returns)) * 10\n",
        "    else:\n",
        "        risk_penalty = 0.0\n",
        "\n",
        "    return -mean_return + risk_penalty\n",
        "\n",
        "# --- RE-INITIALIZE MODEL TO CLEAR THE 'NAN' BRAIN ---\n",
        "model = LSTMRebalancer(input_size=X_train.shape[2], hidden_size=64, output_size=len(tickers))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) # Lower learning rate\n",
        "\n",
        "# --- RE-RUN TRAINING ---\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train)\n",
        "    loss = robust_drawdown_loss(outputs, y_train)\n",
        "\n",
        "    # Check for NaN before moving forward\n",
        "    if torch.isnan(loss):\n",
        "        print(f\"NaN detected at epoch {epoch}\")\n",
        "        break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient Clipping: Prevents the math from exploding\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}\")"
      ],
      "metadata": {
        "id": "Pbp1UJUMY76Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Get AI Predictions for the test set\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    ai_weights = model(X_test).numpy()\n",
        "# 2. Get actual returns for the test period\n",
        "# (The returns of the assets on the days the AI was picking weights for)\n",
        "test_returns = returns.iloc[-len(ai_weights):].values\n",
        "\n",
        "# 3. Calculate Daily Portfolio Returns\n",
        "ai_portfolio_rets = np.sum(ai_weights * test_returns, axis=1)\n",
        "equal_weights = np.array([1/len(tickers)] * len(tickers))\n",
        "eq_portfolio_rets = np.sum(equal_weights * test_returns, axis=1)\n",
        "\n",
        "# Note: We use the weights from Step 3 for the MVO baseline\n",
        "mvo_w = np.array([cleaned_weights[t] for t in tickers])\n",
        "mvo_portfolio_rets = np.sum(mvo_w * test_returns, axis=1)\n",
        "\n",
        "# 4. Calculate Cumulative Growth\n",
        "ai_cum = (1 + ai_portfolio_rets).cumprod()\n",
        "eq_cum = (1 + eq_portfolio_rets).cumprod()\n",
        "mvo_cum = (1 + mvo_portfolio_rets).cumprod()\n",
        "\n",
        "# 5. PLOT THE RESULTS\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ai_cum, label=f'AI Strategy (Drawdown-Optimized)', lw=2, color='blue')\n",
        "plt.plot(mvo_cum, label='Traditional Math (MVO)', lw=1.5, color='green', linestyle='--')\n",
        "plt.plot(eq_cum, label='Equal Weighted (Benchmark)', lw=1, color='gray', alpha=0.7)\n",
        "\n",
        "plt.title('Final Performance Comparison: AI vs Traditional Math', fontsize=14)\n",
        "plt.xlabel('Days in Test Period')\n",
        "plt.ylabel('Cumulative Growth of $1')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# 6. Final Metric Check\n",
        "print(f\"AI Final Return: {(ai_cum[-1]-1)*100:.2f}%\")\n",
        "print(f\"MVO Final Return: {(mvo_cum[-1]-1)*100:.2f}%\")\n",
        "print(f\"AI Max Drawdown: {calculate_max_drawdown(ai_portfolio_rets)*100:.2f}%\")\n",
        "print(f\"MVO Max Drawdown: {calculate_max_drawdown(mvo_portfolio_rets)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "cfOa830NZ5y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "last_ai_weights = ai_weights[-1]\n",
        "mvo_weights_list = [cleaned_weights[t] for t in tickers]\n",
        "\n",
        "weight_df = pd.DataFrame({\n",
        "    'Asset': tickers,\n",
        "    'AI Weights': last_ai_weights,\n",
        "    'MVO Weights': mvo_weights_list\n",
        "})\n",
        "print(weight_df)"
      ],
      "metadata": {
        "id": "crbrqsBCfzsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig('performance_results.pdf', bbox_inches='tight', dpi=300)"
      ],
      "metadata": {
        "id": "ARNjpmT4f9p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def professional_backtest(weights, asset_returns, fee_rate=0.001):\n",
        "    \"\"\"\n",
        "    weights: (N_days, N_assets)\n",
        "    asset_returns: (N_days, N_assets)\n",
        "    fee_rate: 0.1% per trade\n",
        "    \"\"\"\n",
        "    n_days = len(weights)\n",
        "    portfolio_returns = []\n",
        "\n",
        "    # Start with initial weights\n",
        "    current_weights = weights[0]\n",
        "\n",
        "    for t in range(n_days):\n",
        "        # 1. Calculate returns for the day based on current weights\n",
        "        daily_ret = np.sum(weights[t] * asset_returns[t])\n",
        "\n",
        "        # 2. Calculate Transaction Costs\n",
        "        # We pay fees only on the CHANGE in weights (buying/selling)\n",
        "        if t > 0:\n",
        "            weight_change = np.sum(np.abs(weights[t] - weights[t-1]))\n",
        "            transaction_cost = weight_change * fee_rate\n",
        "        else:\n",
        "            transaction_cost = fee_rate # Initial setup cost\n",
        "\n",
        "        portfolio_returns.append(daily_ret - transaction_cost)\n",
        "\n",
        "    return np.array(portfolio_returns)\n",
        "\n",
        "# --- Execute Backtest ---\n",
        "\n",
        "# 1. Calculate Professional Returns\n",
        "ai_pro_rets = professional_backtest(ai_weights, test_returns)\n",
        "mvo_pro_rets = professional_backtest(np.tile(mvo_w, (len(ai_weights), 1)), test_returns)\n",
        "\n",
        "# 2. Calculate Sharpe Ratio (Risk-Adjusted Return)\n",
        "# Formula: (Mean Return / Standard Deviation) * sqrt(252 trading days)\n",
        "def calculate_sharpe(rets):\n",
        "    if len(rets) == 0: return 0\n",
        "    return (np.mean(rets) / np.std(rets)) * np.sqrt(252)\n",
        "\n",
        "# 3. Final Metrics\n",
        "print(f\"--- Professional Results (Post-Fees) ---\")\n",
        "print(f\"AI Final Return: {(np.prod(1 + ai_pro_rets) - 1)*100:.2f}%\")\n",
        "print(f\"MVO Final Return: {(np.prod(1 + mvo_pro_rets) - 1)*100:.2f}%\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"AI Sharpe Ratio: {calculate_sharpe(ai_pro_rets):.2f}\")\n",
        "print(f\"MVO Sharpe Ratio: {calculate_sharpe(mvo_pro_rets):.2f}\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"AI Max Drawdown: {calculate_max_drawdown(ai_pro_rets)*100:.2f}%\")\n",
        "print(f\"MVO Max Drawdown: {calculate_max_drawdown(mvo_pro_rets)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "8ZMojVvGgqB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sortino(rets):\n",
        "    # Only consider negative returns for the denominator\n",
        "    downside_rets = rets[rets < 0]\n",
        "    if len(downside_rets) == 0: return 0\n",
        "    return (np.mean(rets) / np.std(downside_rets)) * np.sqrt(252)\n",
        "\n",
        "print(f\"AI Sortino Ratio: {calculate_sortino(ai_pro_rets):.2f}\")\n",
        "print(f\"MVO Sortino Ratio: {calculate_sortino(mvo_pro_rets):.2f}\")"
      ],
      "metadata": {
        "id": "mN9VXL95hGCY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}